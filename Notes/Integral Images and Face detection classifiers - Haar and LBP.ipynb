{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intergral Image\n",
    "\n",
    "In an integral image the value of pixel (x,y) is equal to the sum of pixels above and to the left of (x,y)\n",
    "\n",
    "<img src=\"src\\integral_image.png\" alt=\"Finding integral image\" width=\"500\"/>\n",
    "\n",
    "### Integral image allows for calculation of sum of all pixels inside a given rectangle using only 4 corner values.\n",
    "\n",
    "In below image, \n",
    "\n",
    "- Point 1 gives sum of all values in block A i.e integral image of Block A\n",
    "- Point 2 gives sum of all values in block A and B i.e integral image of Block A + B\n",
    "- Point 3 gives sum of all values in block A and C i.e integral image of Block A + C\n",
    "- Point 4 gives sum of all values in block A,B,C and D i.e integral image of total block\n",
    "\n",
    "\n",
    "<img src=\"src\\integral_image_use.png\" alt=\"Integral image use\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascade Classifiers\n",
    "\n",
    "Concatenation of a set of weak classifiers to create a strong classifier\n",
    "\n",
    "- Weak classifiers - Limited performance\n",
    "- Strong classifiers - robust performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Haar Classifier is a machine learning based approach, an algorithm created by Paul Viola and Michael Jones; which are trained from many many positive images (with faces) and negatives images (without faces)\n",
    "- It starts by extracting Haar features from each image using windows show below\n",
    "\n",
    "![Fig.1 Haar Windows](src/haar_windows.png \"Haar Windows\")\n",
    "\n",
    "\n",
    "Each window is placed on the picture to calculate a single feature. \n",
    "- This feature is a single value obtained by subtracting the sum of pixels under the white part of the window from the sum of the pixels under the black part of the window.\n",
    "\n",
    "Now, all possible sizes of each window are placed on all possible locations of each image to calculate plenty of features.\n",
    "\n",
    "For example, in above image, we are extracting two features. \n",
    "- The first one focuses on the property that the region of the eyes is often darker than the area of the nose and cheeks. \n",
    "- The second feature relies on the property that the eyes are darker than the bridge of the nose.\n",
    "\n",
    "But among all these features calculated, most of them are irrelevant. \n",
    "\n",
    "For example, when used on the cheek, the windows become irrelevant because none of these areas are darker or lighter than other regions on the cheeks, all sectors here are the same.\n",
    "\n",
    "So we promptly discard irrelevant features and keep only those relevant using called Adaboost.\n",
    "\n",
    "***In the end, the algorithm considers the fact that generally: most of the region in an image is a non-face region. Considering this, it’s a better idea to have a simple method to check if a window is a non-face region, and if it's not-face, discard it right away and don’t process it again. So we can focus mostly on the area where a face is.***\n",
    "\n",
    "\n",
    "https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html\n",
    "\n",
    "https://www.youtube.com/watch?v=_QZLbR67fUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV provides us with a class **cv2.CascadeClassifier** which takes as input the training file of the (Haar/LBP) classifier we want to load and loads it for us.\n",
    "\n",
    "Since we want to load outhe Haar classifier,  its XML training files are stored in the opencv/data/haarcascades/ folder. You can also find them in the data folder of the Github repo\n",
    "\n",
    "To detect face from an image using the CascadeClassifier we use **cv2.detectMultiScale** \n",
    "\n",
    "    cv2.detectMultiScale(image, scaleFactor, minNeighbors): \n",
    "    - This is a general function to detect objects, in this case, it'll detect faces since we called in the face cascade. \n",
    "        - If it finds a face, it returns a list of positions of said face in the form “Rect(x,y,w,h).”, \n",
    "        - if not, then returns “None”.\n",
    "\n",
    "    Image: \n",
    "     - The first input is the grayscale image. So make sure the image is in grayscale.\n",
    "\n",
    "    scaleFactor: \n",
    "     - compensates a false perception in size that occurs when one face appears to be bigger than the other simply because it is closer to the camera.\n",
    "\n",
    "    minNeighbors: \n",
    "    - a detection algorithm that uses a moving window to detect objects, it does so by defining how many objects are found near the current one before it can declare the face found.\n",
    "\n",
    "These are usually used one, there are other parameters as well\n",
    "\n",
    "> Tune these parameters according to the information about data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAAR vs LBP\n",
    "\n",
    "Each OpenCV face detection classifiers has its pros and cons, but the major differences are in accuracy and speed.\n",
    "\n",
    "So, in case more accurate detections are required, Haar classifier is the way to go. This is more suitable in technology such as security systems or high-end stalking.\n",
    "\n",
    "But the LBP classifier is faster, therefore, should be used in mobile applications or embedded systems.\n",
    "\n",
    "Note - both classifiers work with gray scale images only\n",
    "\n",
    "![Fig. 2 HAAR vs LBP](src/haar-vs-lbp.png \"HAAR vs LBP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.superdatascience.com/opencv-face-detection/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Local Binary Patterns, or LBP in short, also needs to be trained on hundreds of images. LBP is a visual/texture descriptor, and thankfully, our faces are also composed of micro visual patterns.\n",
    "\n",
    "So, LBP features are extracted to form a feature vector that classifies a face from a non-face.\n",
    "\n",
    "Each training image is divided into some blocks.\n",
    "For each block, LBP looks at 9 pixels (3×3 window) at a time, and with a particular interest in the pixel located in the center of the window.\n",
    "Then, it compares the central pixel value with every neighbor's pixel value under the 3×3 window. For each neighbor pixel that is greater than or equal to the center pixel, it sets its value to 1, and for the others, it sets them to 0.\n",
    "\n",
    "After that, it reads the updated pixel values (which can be either 0 or 1) in a clockwise order and forms a binary number. Next, it converts the binary number into a decimal number, and that decimal number is the new value of the center pixel. We do this for every pixel in a block.\n",
    "\n",
    "Then, it converts each block values into a histogram, so now we have gotten one histogram for each block in an image.\n",
    "\n",
    "Finally, it concatenates these block histograms to form a one feature vector for one image, which contains all the features we are interested. So, this is how we extract LBP features from a picture.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images are not shown in notebook intentionally\n",
    "\n",
    "Can do plt.imshow() but I've to \n",
    "- change the channels from BGR to RGB to display or \n",
    "- if displaying grayscale image - set cmap to gray \n",
    "\n",
    "Not displaying here as \n",
    "    - I've done it in previous notebook - counting objects notebook; Not interested any more\n",
    "    - I'm developing code in Atom using cv2.imshow() directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ordered_rectangle_points(pts):\n",
    "    '''\n",
    "    Takes an array of 4 co-ordinates and returns a array with consistent ordering of the points for the rectangle\n",
    "\n",
    "    List contains points from top left to bottom left in that order\n",
    "         -> top left, top right, bottom right, bottom left\n",
    "    '''\n",
    "    #initializng a array of coordinates that will be ordered\n",
    "    #such that the first entry is the top-left,the second entry is the top-right,\n",
    "    #the third is the bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4,2), dtype=\"float32\")\n",
    "    #[0]top-left point will have the smallest sum of the co-ordinates\n",
    "    #[2]bottom-right point will have largest sum\n",
    "    #[1]top-right point will have smallest difference between the co-ordinates\n",
    "    #[3]where as bottom-left will have largest diff\n",
    "\n",
    "    p_sum = np.sum(pts,axis=1)\n",
    "    p_diff = np.diff(pts,axis=1)\n",
    "    # From top left to bottom left in that order -> top left, top right, bottom right, bottom left\n",
    "    rect[0] = pts[np.argmin(p_sum)]\n",
    "    rect[1] = pts[np.argmin(p_diff)]\n",
    "    rect[2] = pts[np.argmax(p_sum)]\n",
    "    rect[3] = pts[np.argmax(p_diff)]\n",
    "\n",
    "    return rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def four_point_perspective_transform(image, pts):\n",
    "    '''\n",
    "    Will do a 4 point perspective transform to obtain a top-down, “birds eye view” of an image\n",
    "\n",
    "        Takes input as image and list of four reference points that contain the ROI of the image we want to transform\n",
    "    '''\n",
    "    # obtaining consistent order of points and unpacking them individually\n",
    "    rect = ordered_rectangle_points(pts)\n",
    "    (tl,tr,br,bl) = rect\n",
    "\n",
    "    # computing the width of the new image, which will be the maximum distance between\n",
    "    # bottom-right and bottom-left coordiates or\n",
    "    # top-right and top-left coordinates\n",
    "    widthA = np.sqrt(((tr[0]-tl[0])**2) + ((tr[1]-tl[1])**2)) # tr - tl\n",
    "    widthB = np.sqrt(((br[0]-bl[0])**2) + ((br[1]-bl[1])**2)) # br - bl\n",
    "    #maxWidth = max(widthA,widthB) # returning np.array output\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # computing the height of the new image, which will be the maximum distance between\n",
    "    # top-right and bottom-right coordiates or\n",
    "    # top-left and bottom-left coordinates\n",
    "    heightA = np.sqrt(((tr[0]-br[0])**2) + ((tr[1]-br[1])**2)) # tr - tl\n",
    "    heightB = np.sqrt(((tl[0]-bl[0])**2) + ((tl[1]-bl[1])**2)) # br - bl\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    # Constructing set of destination points to obtain a \"birds eye view\" (i.e. top-down view) of the image,\n",
    "    # again specifying points in the top-left, top-right, bottom-right, and bottom-left order\n",
    "    # We get the dimensions of the new image based on the width and height calculated\n",
    "    dest = np.array([[0, 0], [maxWidth-1, 0], [maxWidth-1, maxHeight-1], [0, maxHeight-1]], dtype=\"float32\")\n",
    "    # making it float32 as getPerspectiveTransform requires it\n",
    "\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    transformation_matrix = cv2.getPerspectiveTransform(rect, dest)\n",
    "    warped = cv2.warpPerspective(image, transformation_matrix, (maxWidth, maxHeight))\n",
    "\n",
    "    # return warped image\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.getPerspectiveTransform??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually obtain the top-down, “birds eye view” of the image we’ll utilize the cv2.getPerspectiveTransform  function. \n",
    "    - This function requires two arguments, rect and dst\n",
    "        - rect is Coordinates of quadrangle vertices in the source image.\n",
    "        - dst is Coordinates of the corresponding quadrangle vertices in the destination image.\n",
    "        - getPerspectiveTransform requires float32 \n",
    "\n",
    "The cv2.getPerspectiveTransform  function returns a matrix , which is the actual transformation matrix.\n",
    "\n",
    "We apply the transformation matrix using the cv2.warpPerspective function. We pass in the image , our transform matrix , along with the width and height of our output image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection\n",
    "\n",
    "The first step to building our document scanner app using OpenCV is to perform edge detection. Let’s take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file='scan_4.jpg'\n",
    "image = cv2.imread(file)\n",
    "original = image.copy()\n",
    "#In order to speedup image processing, as well as make our edge detection step more accurate resizing image\n",
    "image = imutils.resize(image, height=500)\n",
    "# To scale back the image, if required when displaying the output\n",
    "aspect_ratio = original.shape[0] / 500.0\n",
    "\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#  perform Gaussian blurring to remove high frequency noise (aiding in contour detection), and performing Canny edge detection.\n",
    "gray = cv2.GaussianBlur(gray,(5,5),0)\n",
    "edged = cv2.Canny(gray, 5,100)\n",
    "\n",
    "##print(\"Edge detection \")\n",
    "##cv2.imshow(\"Blurred Gray\", gray)\n",
    "#cv2.imshow(\"Edged\", edged)\n",
    "#cv2.waitKey(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding contours\n",
    " - sorting the contours by area and keep only the largest ones. This allows us to only examine the largest of the contours, discarding the rest.\n",
    " \n",
    "#### Contour Area\n",
    "- cv2.contourArea\n",
    "    - Contour area is given by the function cv2.contourArea() or from moments, M['m00'].\n",
    "\n",
    "#### Contour Perimeter\n",
    "- cv2.arcLength(contour, True)\n",
    "    - It is also called arc length. It can be found out using cv2.arcLength() function. Second argument specify whether shape is a closed contour (if passed True), or just a curve\n",
    "    \n",
    "#### Contour Approximation\n",
    "- cv2.approxPolyDP(c, 0.02 * perimeter, True)\n",
    "    - It approximates a contour shape to another shape with less number of vertices depending upon the precision we specify. It is an implementation of Douglas-Peucker algorithm. Check the wikipedia page for algorithm and demonstration.\n",
    "\n",
    "    - To understand this, suppose you are trying to find a square in an image, but due to some problems in the image, you didn't get a perfect square, but a \"bad shape\". Now you can use this function to approximate the shape. \n",
    "    - ***In this, second argument is called epsilon, which is maximum distance from contour to approximated contour. It is an accuracy parameter. A wise selection of epsilon is needed to get the correct output. *** Here 0.02*perimeter implies it is 2% of the arclength\n",
    "    - Third argument specifies whether curve is closed or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding contours\n",
    "cnts_mat = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts_mat[1] # as we have cv2 version 3.4\n",
    "\n",
    "#sorting the contours by area and keep only the largest ones. This allows us to only examine the largest of the contours, discarding the rest.\n",
    "cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5]\n",
    "for c in cnts:\n",
    "    perimeter = cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, 0.02 * perimeter, True)\n",
    "    # if our approximated contour has four points, then we\n",
    "    # can assume that we have found our surface\n",
    "    if len(approx) == 4:\n",
    "        op_Cnt = approx\n",
    "        break\n",
    "\n",
    "##print(\"Finding contours of object\")\n",
    "#cv2.drawContours(image, [op_Cnt], -1, (0, 255, 0), 2)\n",
    "#cv2.imshow(\"Outline\", image)\n",
    "#cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### threshold function\n",
    "The scikit-image adaptive threshold function is more powerful than the OpenCV one. It includes more than just Gaussian and mean, it includes support for custom filtering along with median (Although I only use Gaussian for this example). I also found it substantially easier to use than the OpenCV variant. In general, I just (personally) like the scikit-image version more.\n",
    "\n",
    "\n",
    "If somebody wants to use the opencv threshold I think this is an equivalent substitute:\n",
    "\n",
    "warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 251, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'op_Cnt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-fc3b7778754f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwarped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfour_point_perspective_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_Cnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwarped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreshold_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gaussian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwarped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwarped\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"uint8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m##print(\"Applying perspective transform\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'op_Cnt' is not defined"
     ]
    }
   ],
   "source": [
    "warped = four_point_perspective_transform(image, op_Cnt.reshape(4, 2))\n",
    "warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "T = threshold_local(warped, 11, offset = 10, method = \"gaussian\")\n",
    "warped = (warped > T).astype(\"uint8\") * 255\n",
    "##print(\"Applying perspective transform\")\n",
    "#cv2.imshow(\"Scanned\", warped)\n",
    "#cv2.waitKey(3000)\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If We are getting _NameError: name 'op_Cnt' is not defined_ It means that there is no contour with 4 sides. \n",
    "\n",
    "That is most probably our code is not able to detect edges correctly or the image passed does not fit our current code.\n",
    "- Can add try and exept block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try, \n",
    "given the transformation matrix M you can calculate the mapped location of any source image pixel (x,y) (or a range of pixels) using:\n",
    "dest(x) = [M11x + M12y + M13]/[M31x + M32y + M33]\n",
    "dest(y) = [M21x + M22y + M23]/[M31x + M32y + M33]\n",
    "\n",
    "Why bother?\n",
    "I used this method to map a laser pointer from a keystoned camera image of a large screen image back onto the original image…allowing me to “draw” on the large screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "\n",
    "In Live feed camera - \n",
    "You can eliminate motion blur using a really good camera sensor, and manual control of white balance. This is critical since a lot of sensors come with a controller which by default will try to increase the brightness of the image by capturing multiple frames in succession and adding them together. This process is what creates motion blur so you need to simply disable automatic white balance in the controller, and you’ll get clean frames every time. However this also means that in some situations it will be too dark for the sensor to see anything. One way to solve this is to put a large amount of powerful infrared LED lights around or behind the sensor, and remove the infrared filter from the sensor so it becomes sensitive to infrared light. The sensor will not see colors, but for reading text from a page you don’t need colors. This way your sensor will see images even in total “darkness” without blinding the non-blind with a potentially strong white light. Reach out to me if you’re interested and I will send you information about such a sensor that we use in my company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
